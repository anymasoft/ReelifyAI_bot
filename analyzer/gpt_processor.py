import openai
import hashlib
import json
from typing import Dict, List
import logging
from analyzer.ngram import NGramAnalyzer
from storage.redis import RedisStorage
from fuzzywuzzy import fuzz

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GPTProcessor:
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.redis = RedisStorage()
        self.analyzer = NGramAnalyzer()

    def cluster_phrases(self, phrases: List[Dict[str, int]]) -> List[Dict[str, int]]:
        clustered = []
        used = set()
        for i, p1 in enumerate(phrases):
            if i in used:
                continue
            cluster = [p1]
            cluster_count = p1['count']
            for j, p2 in enumerate(phrases[i+1:], i+1):
                if j not in used and fuzz.ratio(p1['phrase'].lower(), p2['phrase'].lower()) > 80:
                    cluster.append(p2)
                    cluster_count += p2['count']
                    used.add(j)
            clustered.append({"phrase": p1['phrase'], "count": cluster_count})
            used.add(i)
        return clustered

    def filter_junk_phrases(self, phrases: List[Dict[str, int]], niche: str) -> List[Dict[str, int]]:
        """Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¼ÑƒÑÐ¾Ñ€Ð½Ñ‹Ñ… Ñ„Ñ€Ð°Ð· Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ GPT."""
        phrase_list = [p['phrase'] for p in phrases]
        prompt = """
        Ð’Ñ‹ â€” SEO-Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº. Ð”Ð°Ð½Ð° Ð½Ð¸ÑˆÐ°: "{}".
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚Ðµ, ÐºÐ°ÐºÐ¸Ðµ Ð¸Ð· ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… Ñ„Ñ€Ð°Ð· Ð±ÐµÑÐ¿Ð¾Ð»ÐµÐ·Ð½Ñ‹ Ð´Ð»Ñ SEO-Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð² ÑÑ‚Ð¾Ð¹ Ð½Ð¸ÑˆÐµ.
        Ð‘ÐµÑÐ¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹ Ð²ÐºÐ»ÑŽÑ‡Ð°ÑŽÑ‚:
        - Ð ÐµÐºÐ»Ð°Ð¼Ð½Ñ‹Ðµ Ð¼Ð°Ñ€ÐºÐµÑ€Ñ‹ ("ÑÐºÐ¸Ð´ÐºÐ°", "Ð°ÐºÑ†Ð¸Ñ", "ðŸ”¥").
        - ÐžÐ±Ñ‰Ð¸Ðµ ÑÐ»Ð¾Ð²Ð° Ð±ÐµÐ· ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð¸ÐºÐ¸ ("ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾", "Ð»ÑƒÑ‡ÑˆÐ¸Ð¹").
        - Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¸Ð»Ð¸ Ð½ÐµÐ¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹ ("Ð¾Ð±ÑƒÐ²Ð½Ð°Ñ ÑÐµÑ€Ð¸Ñ", "Ð² Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ð¸").
        - Ð‘Ñ€ÐµÐ½Ð´Ñ‹ ("ZOLINBERG", "YZYNX").
        Ð’ÐµÑ€Ð½Ð¸Ñ‚Ðµ JSON: [{{ "phrase": "Ñ„Ñ€Ð°Ð·Ð°", "is_junk": true/false }}, ...].
        Ð¤Ñ€Ð°Ð·Ñ‹:
        {}
        """.format(niche, "\n".join(phrase_list))
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.1
            )
            result = json.loads(response.choices[0].message.content)
            return [
                p for p in phrases
                if not next((r['is_junk'] for r in result if r['phrase'] == p['phrase']), False)
            ]
        except Exception as e:
            logger.error(f"GPT junk filter error: {str(e)}")
            return phrases  # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ

    def process_ngrams(self, query: str, data: Dict[str, List[str]]) -> List[Dict[str, int]]:
        cache_key = f"gpt_ngrams:{hashlib.md5((query + json.dumps(data)).encode()).hexdigest()}"
        cached = self.redis.get_cache(0, cache_key)
        if cached:
            logger.info(f"Cache hit for GPT n-grams: {cache_key}")
            return cached

        texts = (
            data.get('titles', []) +
            data.get('descriptions', []) +
            data.get('alt_texts', []) +
            data.get('breadcrumbs', []) +
            data.get('product_descriptions', [])
        )
        text_input = "\n".join([t for t in texts if t and t.lower() not in ['Ñ€Ð°ÑÐ¿Ñ€Ð¾Ð´Ð°Ð¶Ð°', 'Ð¾ÑÑ‚Ð°Ð»Ð°ÑÑŒ 1 ÑˆÑ‚', '']])
        if not text_input:
            logger.warning("Empty or non-informative text input for GPT processing")
            return self.fallback_ngram_analysis(query, data)

        prompt = """
        Ð’Ñ‹ â€” SEO-Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº. Ð”Ð°Ð½ Ð·Ð°Ð¿Ñ€Ð¾Ñ: "{}".
        Ð˜Ð·Ð²Ð»ÐµÐºÐ¸Ñ‚Ðµ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð° ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹ (1â€“3 ÑÐ»Ð¾Ð²Ð°), Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ð´Ð»Ñ SEO.
        ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð²ÐºÐ»ÑŽÑ‡Ð¸Ñ‚Ðµ Ð±Ð¸Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "Ð·Ð¸Ð¼Ð½Ð¸Ðµ Ð±Ð¾Ñ‚Ð¸Ð½ÐºÐ¸") Ð¸ Ñ‚Ñ€Ð¸Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "Ð¶ÐµÐ½ÑÐºÐ¸Ðµ Ð·Ð¸Ð¼Ð½Ð¸Ðµ Ð±Ð¾Ñ‚Ð¸Ð½ÐºÐ¸"), ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð¼.
        Ð˜ÑÐºÐ»ÑŽÑ‡Ð¸Ñ‚Ðµ:
        - ÐÐµÐ¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° ("ÐºÑƒÐ¿Ð¸Ñ‚ÑŒ", "Ð´Ð¾ÑÑ‚Ð°Ð²ÐºÐ°", "Ñ€Ð°ÑÐ¿Ñ€Ð¾Ð´Ð°Ð¶Ð°", "Ð¾Ð±ÑƒÐ²Ð½Ð°Ñ ÑÐµÑ€Ð¸Ñ", "Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ", "Ð¾Ñ‚Ð·Ñ‹Ð²Ñ‹").
        - Ð‘Ñ€ÐµÐ½Ð´Ñ‹ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "ZOLINBERG", "YZYNX", "Vicappy", "ARPSTAR", "Your Way").
        - Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÐºÐ¾Ð´Ñ‹ ("ma7e4zm/a").
        - ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ»Ð¾Ð² ("Ð±Ð¾Ñ‚Ð¸Ð½ÐºÐ¸ Ð±Ð¾Ñ‚Ð¸Ð½ÐºÐ¸").
        ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹ ("boots" â†’ "Ð±Ð¾Ñ‚Ð¸Ð½ÐºÐ¸", "women" â†’ "Ð¶ÐµÐ½ÑÐºÐ¸Ðµ").
        Ð’ÐµÑ€Ð½Ð¸Ñ‚Ðµ JSON: [{{ "phrase": "Ñ„Ñ€Ð°Ð·Ð°", "count": N }}, ...], Ð³Ð´Ðµ N â€” Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð° Ñ„Ñ€Ð°Ð·Ñ‹.
        Ð¢ÐµÐºÑÑ‚:
        {}
        """.format(query, text_input)
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.1
            )
            result = response.choices[0].message.content
            parsed_result = json.loads(result)
            parsed_result = [item for item in parsed_result if item.get('phrase') and item.get('count', 0) > 0]
            if not parsed_result:
                logger.warning("GPT returned empty result, falling back to local n-gram analysis")
                parsed_result = self.fallback_ngram_analysis(query, data)
            clustered_result = self.cluster_phrases(parsed_result)
            filtered_result = self.filter_junk_phrases(clustered_result, query)
            self.redis.set_cache(0, cache_key, filtered_result)
            return filtered_result
        except Exception as e:
            logger.error(f"GPT processing error: {str(e)}")
            return self.fallback_ngram_analysis(query, data)

    def fallback_ngram_analysis(self, query: str, data: Dict[str, List[str]]) -> List[Dict[str, int]]:
        texts = (
            data.get('titles', []) +
            data.get('descriptions', []) +
            data.get('alt_texts', []) +
            data.get('breadcrumbs', []) +
            data.get('product_descriptions', [])
        )
        tfidf_result = self.analyzer.analyze_tfidf(texts, ngram_range=(1, 3), max_features=50)
        return [{"phrase": phrase, "count": int(score * 10)} for phrase, score in tfidf_result]